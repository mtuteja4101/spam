# -*- coding: utf-8 -*-
"""spammodel.ipynb

Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1lLiPN4xANQ3jh1DDmfaMG1vk5GkPP7qi
"""

import pickle
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
import requests
#from sentence_transformers import SentenceTransformer

#!pip install sentence-transformers

#sentence_bert_model = SentenceTransformer('paraphrase-MiniLM-L3-v2')

nltk.download('stopwords')

#messages = pd.read_csv("SMSSpamCollection", sep='\t', names=["label", "message"])

messages = pd.read_csv("spam_dataset.csv", names = ["label", "message"]).iloc[1:,:]

messages.head()



#trying to get only a certain number of hams as spams messages are way too less
b = messages[messages["label"] == 'spam']
a = messages[messages["label"] == 'ham']#.iloc[:1000,:]

shuffled_ham = a.sample(frac=1)
shuffled_ham = shuffled_ham.iloc[:10000,:]
dataset_ab = pd.concat([shuffled_ham,b], axis=0)
shuffled_set = dataset_ab.sample(frac=1)
shuffled_set.reset_index(inplace=True, drop = True)

shuffled_set

#messages['label'].value_counts().plot.bar()
shuffled_set.label.value_counts().plot.bar()

corpus =[]
for i in shuffled_set.iloc[:,1]:
  review = re.sub('[^a-zA-Z]',' ',i)
  review = review.lower()
  review = review.split()

  review = ' '.join(j for j in review if j not in stopwords.words('english'))

  corpus.append(review)

cv = CountVectorizer()
x = cv.fit_transform(corpus).toarray()

pickle.dump(cv, open('transform.pkl','wb'))

y = pd.get_dummies(shuffled_set['label'])
#or we can use this

#messages['label'] = messages.label.apply(lambda x: '1' if x == 'ham' else '0')

y = y.iloc[:,1]

xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size=0.2, random_state=0)

x.shape

spam_model = MultinomialNB().fit(xtrain, ytrain)

pickle.dump(spam_model, open('model.pkl','wb'))

xtrain.shape

ypred = spam_model.predict(xtest)

confusion_m = confusion_matrix(ytest,ypred)
confusion_m

accuracy = accuracy_score(ytest, ypred)
accuracy

pickle.dump(spam_model, open('model.pkl','wb'))

#trying sentence bert embedding

'''
sentence_corpus = []

for i in shuffled_set.iloc[:,1]:
  sen_encoding = sentence_bert_model.encode(i)

  sentence_corpus.append(sen_encoding)

y = pd.get_dummies(shuffled_set['label'])
y = y.iloc[:,1]

xtrain, xtest, ytrain, ytest= train_test_split(sentence_corpus, y, test_size=0.2, random_state=0)

#spam_sent_model = MultinomialNB().fit(xtrain, ytrain)

ypred = spam_sent_model.predict(xtest)
confusion_m = confusion_matrix(ytest,ypred)
confusion_m

accuracy = accuracy_score(ytest, ypred)
accuracy
'''